### Re-creating Shazam Music Algorithm from Scratch

- This is not a normal readme file. It's more of a note-taking readme where I'll jot down
  the details regarding what I am doing, how features I've implemented and what I learned
- The debugging processes, and as the project scales in complexity these notes will help me keep track of the things that are required to stay on course to completing this project.
- No AI assistance for code will be taken. Every single piece of code in this project will be thought thoroughly and written by me.

## 21st September 2025 - 3:07 p.m.

- Need to thing through the file structure for this project.
  - Let's write down the requirements that are needed to make this project.
    1. The Recording/Uploading of songs and clips
    2. The shazam magic 
    3. The Matching process
    4. The output 

- Can use FFMEG to process MP3 files and also record clips. 
- For the clip it's going to be something around 10 sec.
- But the song can vary from 2 mins to 5 mins, Therefore it's better to divide a song into chunks of 10 seconds, identify the pairs of frequency where amplitude changes rapidly and store them into the database.
- A challenge: what if the clip we have is in between two chunks of our song? 
- We can identify the interesting points in the clip;
- make chunks out of the points identified.
- generate hashes out of those chunks
- search the database for the hashes that match our clip hashes.
- return the songs with the max no. of hash matches.

- For songs we process each song in chunks, then we generate peak points,
then we batch the peak points into another chunk and generate a hash for that chunk and store it in the db.

- Another important thing to note is the time offset searching for the hashes. The hashes should be generated by keeping the time offset in mind, the lower the time offset of a pair of clip hash and song hash, the higher the probability of it being the correct song.

- For any song upload we are going to convert it into wav format. This ensures standardization of the whole process for the shazam algo.


## 27th September 2025 - 12:48 p.m - 1:06 p.m. 

- Will start by creating a record feature for the clip, a upload feature for the song, then a converter to wav for any format to wav and then wav processing for our Shazam Magic Pipeline.

- Step 1 : Installing FFMPEG (done)

- Step 2 : Figure out how to use FFMEG to Store a 10 second clip in memory. 
  - 28th Sept - 11:00 p.m. - 11:31 p.m.
           - Faced with a design choice now, Should I create a 10 sec buffer audio clip or store it properly inside a physical location. 
           - Buffer one seems to me the correct choice cause when the algo is going through the clip it can't listen to any other clip until the whole process is done.

- Step 3 : Now try creating a format change function using ffmeg to convert a given clip or song (used for both) into WAV (Next task)

## Winter Arc Begins - 1st Oct 2025 5:18 P.M (In mumbai ;)

- After a heavy look up into similar projects I now understand that the listening part of the project will be done on the client side and therefore I will have to implement the listening thing when I create the frontend of the project. 

- For backend we can assume that the frontend will be sending us audio data and then we will have to convert that into WAV format.

- So My first task is to complete the transformation of reading and writing raw audio data into WAV Format and saving them.

- In my understanding of the code base I learnt that FFMPED is a cross platform conversion library that is useful to convert files not record them. 

- We can use different libraries for recording and then convert it properly into different format using FFMPEG. It's like photoshop but in a code base.

- Main Package named "shazoom" ;)

- completed a convert function that takes in recording data sent by frontend and turns it into WAV format file and stores it and return the directory of the output file.

## 2nd Oct 2025 4:05 P.M (in mumbai)
- Now I need to create functions to extract data (metadata like sample rate, channels, bits per sample) from WAV files to use for our song recog algo.

- Creating wav.go, defining structure of WAV header (for writing and parsing raw WAV files)
  - WriteWAVHeader() for metadata
  - WriteWAVFile() for PCMData Extraction. (will use ffprobe [Part of FFMPEG] for metadata extraction)
  - ProcessRecording -> Will take the raw data provided by the frontend, will convert the file format to wav (build earlier), then will process the metadata and then create sample (float64) and return them for fingerprinting process.

- Alright completed the WAV header structure and writing to header. 

## 3rd Oct 2025 4:00 P.M.
 
- Completed the wav file writing function and also the extraction of wav info from filepath function. (required during wav processing)

- Completed the extraction and normalization into float64 from raw PCM data (convert to signed 16 bit integer and then normalized into float64 [-1.0 to 1.0]) (required during wav processing)

- Next step is to complete the wav processing by adding the metadata extraction from raw file and then completing the whole flow in the final processRecording function. [6:07 p.m]

## 5th Oct 2025 8:30 P.M.

- Completed the processing of an incoming file using ffprobe.
- Tested the function to see if the processing of the song details are happening correctly [passed]
- Now going to complete the processing of the song function that takes in a file and then process it correctly for the main shazam function


## 23rd Oct 2025 4:07 P.M.

- After a long break, figuring out what did I do last time round and what was I planning to do. Going through this readme file is very helpful. Though I believe I have not noted down what needs to be done once I come back.

- Going through the commit history, I have completed the processing part of any audio clip or song. Now I can start working on the actual shazoom algorithm.

- So we require basically a spectrogram of the song. Heat signatures which can then produce anchor points in the songs as well as the sample which will be sent to us. 

- We can then based on those anchor points compare the relative timing between the sample's anchor times and the song anchor times, and give score to each. 

- Basically: 
            1. Analize the audio sample given (spectrogram -> peaks -> fingerprinting). 
            2. use the fingerprints to find matching song in DB. (match timestamps & targetZones with songs)
            3. Filter matches based on relative timing of anchor b/w sample and song.
            4. Assign scores to these potential matches and return the list of matches in decreasing order.
            5. show these on the client side as matches: 1. 2. 3. (etc)

- Working on creating the spectrogram -> peaks -> fingerprinting pipeline.
> Choosing hanning or hamming window for spectral leakage (reference: [https://www.youtube.com/watch?v=PiFFY3CtKmw&t=389s])
- Alright completed Low filter function today for the spectrogram. 
- Need to complete the downsample function so the sample is finally ready for the spectrogram analysis.

## 24th Oct 2025 3:07 P.M.

- Starting work on downsample function which will basically reduce the number of samples we have from 44k to 11k. This is essential to make sure I only use the important samples for fingerprinting and not use all the different noise present in the sample/song.

- Completed all the functions required.

## 28th Oct 2025 9:43 P.M.

- Conversion of Spectrogram to Heatmap to pinpoint the anchor points.

- Testing Logs of the pipeline created so far with dummy data (run go test ./test -v)
=== RUN   TestFullPipeline
    integration_test.go:38: Generated spectrogram with 22 time windows and 1024 frequency bins
    integration_test.go:48: Extracted 44 peaks from spectrogram
--- PASS: TestFullPipeline (0.57s)
=== RUN   TestPipelineWithSilence
--- PASS: TestPipelineWithSilence (0.14s)
=== RUN   TestPipelineWithWhiteNoise
    integration_test.go:114: White noise produced 11 peaks
--- PASS: TestPipelineWithWhiteNoise (0.14s)
=== RUN   TestPipelineWithChirp
    integration_test.go:152: Chirp signal produced 22 peaks
--- PASS: TestPipelineWithChirp (0.56s)
--- PASS: TestComponentIntegration (0.28s)
    --- PASS: TestComponentIntegration/LowPassFilter (0.00s)
    --- PASS: TestComponentIntegration/Downsample (0.14s)
    --- PASS: TestComponentIntegration/FFT (0.00s)
    --- PASS: TestComponentIntegration/FilterThenDownsample (0.14s)
=== RUN   TestPipelineConsistency
--- PASS: TestPipelineConsistency (0.28s)

## 5nd Nov 2025 11.21 P.M.

- Working on getting the peaks and setting up anchor points

## 13-14th Nov 2025 11.00 A.M - 12:28 A.M

- Completed the redundant Conversion to Wav and Reformat Wav function into one.

## 15 Nov 2025 5:05 P.M 

- spent time getting the wav - conversion to samples pipleline tested, found bugs and did remediation required.
- For detailed debugging documentation, see [DEBUGGING.md](./DEBUGGING.md)


## 4th Dec 2025 (while flying :) 

- spent time going through the test build, & need to generate test which can test the whole process.

## 5th Dec 2025 

- Need to create test for the whole pipeline and start building the fingerprinting process right after

â¯ go test -v
=== RUN   TestFullPipeline
    integration_test.go:13: Starting TestFullPipeline with real audio data.
    common.go:75: Successfully processed 4058168 samples from 92.02 second recording
    integration_test.go:32: Truncating audio to 441000 samples (10.0s) for testing.
    integration_test.go:35: Successfully fetched 441000 samples at 44100 Hz (10.00s duration)
    integration_test.go:50: Generated spectrogram with 111 time windows and 1024 frequency bins
    integration_test.go:62: Extracted 111 peaks from spectrogram
--- PASS: TestFullPipeline (14.40s)
**PASS**
**ok      shazoom/test    14.862s**

- Awesome, so now that the spectrogram is working.

## 6th Dec 2025 

- TODO LIST 
    * [X] Complete the fingerprinting prococess to generate hashes (Till now we are just creating a heatmap)
    * [X] Complete Address function that creates a unique hash that will be used in fingerprinting
    * [X] Test the fingerprinting process now with the hashes. 

- Address formula 
  * ![Formula](image.png)

- Test Log: 
=== RUN   TestFullPipeline
    integration_test.go:12: Starting TestFullPipeline with real audio data.
    common.go:75: Successfully processed 4058168 samples from 92.02 second recording
    integration_test.go:27: Truncating audio to 441000 samples (10.0s) for testing.
    integration_test.go:30: Successfully fetched 441000 samples at 44100 Hz (10.00s duration)
    integration_test.go:45: Generated spectrogram with 214 time windows and 512 frequency bins
    integration_test.go:57: Extracted 522 peaks from spectrogram
    integration_test.go:104: Generated 3565 total fingerprints. Logging 5 sample hashes:
    integration_test.go:107: Sample Hash #1: 0x5D81405C (Decimal: 1568751708)
    integration_test.go:107: Sample Hash #2: 0x170B005C (Decimal: 386596956)
    integration_test.go:107: Sample Hash #3: 0x0289C0B9 (Decimal: 42582201)
    integration_test.go:107: Sample Hash #4: 0x2E8100B9 (Decimal: 780206265)
    integration_test.go:107: Sample Hash #5: 0x0182005C (Decimal: 25296988)
--- PASS: TestFullPipeline (0.55s)
**PASS**
**ok      shazoom/test    1.171s**

- Awesome news, fingerprinting works. Next time I am here, we are doing matching and testing it. Then we will move to the frontend!

## 7th Dec 2025

- Time to create matching functions (Main logic and integrate DB)
- Looking at options to setup the cloud database (Firestore, CloudSQL, NeonDB)
- Since my goal is to deploy this application for everyone to use I need to think in terms of architecture within GCP environment.
- Awesome news -> Just created CloudSQL(PostgreSQL) server on GCP. Using the enterprise plus addition as it is free for 30 days. Perfect for my application and use since in 30 days I will run out of credits (lol). 
- Configured the CloudSQL 
- Creating Data base schema.

- Matches datastructure format: 
    * $$\text{matches} = \{ \underbrace{\text{Song ID}}_{\text{uint32 Key}}: [\underbrace{[\text{Sample Time}, \text{DB Time}]}_{\text{Match Pair 1}},\underbrace{[\text{Sample Time}, \text{DB Time}]}_{\text{Match Pair 2}}, ...] \}$$

- Figure out which type of data structure will be required optimally do the fingerprinting with 

## 8th Dec 2025
- 1. Working on getting the matches logic completed [x]
- 2. Trying to establish connection with the CloudSQL Cluster.[x]
- 3. Creating Test suit for the creation of tables [x]

TODO: For tom
- The connections works now so I am confident in implementing a test in which we can process a whole song and store fingerprints on CloudSQL.
- Implement that test and then we can move to the front-end of the project.
- The test will help us in integrating parts of the front-end with backend.
- Will have to create WASM as well for that so delve into the architecture.

- Test Log: 
  === RUN   TestNewPostgresClient
  successfully created postgreSQL client and created tables    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:77: Successfully connected, tables created, and database is queryable. Total songs: 0
  --- PASS: TestNewPostgresClient (0.54s)
  PASS shazoom/test  1.021s

## 9th Dec 2025

- Tested the fingerprints generation with DB (Success!)
  - Test Logs (only the last parts): 
    *     /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:179: Successfully stored fingerprints to DB :) --- PASS: TestNewPostgresClient (1.90s) PASS-2.387s

- Last test for backend (MAIN TEST)
  * Build a test on shazoom to listen to a 10-15 second clip then utilize the shazoom functions to match and figure out the song 

- A problem that I ran into: The number of fingerprints generated per peaks are pretty low. But I am taking the decision that it's okay for now as we are not making a commercial application. But ideally per song there should be 25-30k fingerprints. My algo is producing only 3k-5k.

- Fixed. The issue was in passing the correct data to the pipeline. I was processing the mp3 and then passing that mp3 only :DUMB: but yeah. I created another function to directly pass the samples, and now I am getting good enough fingerprints.

- Test Log: 
  === RUN   TestNewPostgresClient
successfully created postgreSQL client and created tables    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:74: Successfully registered song to DB with Song ID: 1008504485
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:77: Starting TestFullPipeline with real audio data.
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/common.go:75: Successfully processed 11648053 samples from 264.13 second recording
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:85: Successfully fetched 11648053 samples at 44100 Hz (264.13s duration)
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:100: Generated spectrogram with 5686 time windows and 512 frequency bins
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:112: Extracted 13603 peaks from spectrogram
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:156: Generated 49362 total fingerprints. Logging 5 sample hashes:
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:159: Sample Hash #1: 0x228B8145 (Decimal: 579567941)
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:159: Sample Hash #2: 0x13998000 (Decimal: 328826880)
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:159: Sample Hash #3: 0x2393422D (Decimal: 596853293)
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:159: Sample Hash #4: 0x068BC22D (Decimal: 109822509)
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:159: Sample Hash #5: 0x130501D0 (Decimal: 319095248)
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/db_client_test.go:167: Successfully stored fingerprints to DB :)
--- PASS: TestNewPostgresClient (4.92s)
PASS
ok      shazoom/test    5.133s (YIPEE)

- Fixed it using batch inserting (2000) at a time, still able to process 260s length song with 49k fingerprints in under 5 seconds!

## 11th Dec 2025

- Skipped a day so sorry for that :(
- Will test the matching today by creating a small clip recorder and then try to match it with the query logic written. Let's see how today goes. 
- If all goes well then I will be done with the backend logic. 
- I will complete today by deciding on the frontend to use and then I will create a repo that will share both the front end and then backend. 
- I will be using web sockets to relay information between backend and frontend. This is because HTTP servers have a big overhead (headers). Web sockets after the inital handshake are a very lightweight protocol. 
- Web sockets maintain the connection so data transfer can happen back and forth in real time whereas HTTP creates a new connection for every request (request response model). 
- Since our audio processing and sample recording will be done on the client side. We require a constant connection with the backend to relay information in order for a smooth user experience. 
- For reasons stated, I have decided to go with **websockets**.

- Failed in testing. Casting issue between my application layer and database layer. Trying to fix that.

- Test Logs: 
  === RUN   TestMatching
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/match_test.go:68: Recording for the next 10 seconds
Total size of recorded sample data, 881574/nGenerated 5702 fingerprints from the recorded sample.
successfully created postgreSQL client and created tables
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/match_test.go:126: Successfully found 3 matches, in 1.101181666s time
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/match_test.go:141: Failed to match with the expected title: Tum Se
--- FAIL: TestMatching (11.48s)
FAIL
FAIL    shazoom/test    12.039s

- Per observervation: The matching algo is not working, will need to pinpoint what's going wrong and fix it. 
- TODO: Find out what's going wrong and where the matching is giving out the wrong results. (Could be the pipeline, could be the scoring logic.) DEBUG EVERYTHING!

- Well I GOT IT TO WORK! SO HAPPY !!!

- TEST LOG: 
=== RUN   TestMatching
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/match_test.go:54: Recording for the next 10 seconds (Fresh Sample)...
Total size of recorded sample  881574
Generated 867 fingerprints from the recorded sample.
successfully created postgreSQL client and created tables
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/match_test.go:115: Successfully found 3 matches, in 925.90925ms time
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/match_test.go:142: SUCCESS: Matched 'Le Aaunga' by Arijit singh (Score: 18.00)
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/match_test.go:99: Cleanup: Deleted reformatted .wav file
    /Users/prayushgiri/Projects/Shazm Music Algorithm Project/test/match_test.go:89: Cleanup: Deleted raw_recording.wav
--- PASS: TestMatching (11.33s)
PASS
ok      shazoom/test    11.876s

## 15th Dec 2025

- Going to choose a framework for frontend